# Quantized CNV (Compact Neural Vision) on CIFAR-10
# 4-bit quantization-aware training (QAT) using Brevitas
# Expected accuracy: ~85-88% after 200 epochs

seed:
  enabled: true
  value: 42
  deterministic: false

dataset:
  name: "cifar10"
  root: "./data"
  download: true
  num_workers: 4
  # val_split: 0.1

model:
  name: "quant_cnv"

# Quantization settings (Brevitas)
quantization:
  in_weight_bit_width: 8  # Bit width for input quantization
  weight_bit_width: 2   # Bit width for weight quantization
  act_bit_width: 2      # Bit width for activation quantization

loss:
  name: "sqr_hinge"

training:
  batch_size: 64
  epochs: 300

optimizer:
  name: "adam"
  learning_rate: 0.01
  momentum: 0.9
  weight_decay: 0.0

scheduler:
  name: "cosine"
  T_max: 300
  warmup_epochs: 0

amp:
  enabled: false  # Disable AMP for quantized training

tensorboard:
  enabled: true

progress:
  enabled: true

checkpoint:
  enabled: true
  dir: "./experiments"
  save_frequency: 50
  save_best: true
