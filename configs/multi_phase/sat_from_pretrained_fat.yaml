# FAT to SAT Configuration
# Load a pre-trained FAT model and continue training with SAT (no fault injection)

# Seed configuration
seed:
  enabled: true
  value: 42
  deterministic: false

# Dataset configuration
dataset:
  name: "cifar10"
  root: "./data"
  download: true
  num_workers: 4

# Model configuration
model:
  name: "quant_cnv"

# Quantization settings
quantization:
  in_weight_bit_width: 8
  weight_bit_width: 8
  act_bit_width: 8

# Loss function
loss:
  name: "sqr_hinge"

# Training configuration
training:
  batch_size: 256

# Optimizer configuration
optimizer:
  name: "adam"
  learning_rate: 0.0001  # Low LR for continuing from FAT model
  weight_decay: 0.0

# Scheduler configuration
scheduler:
  name: "cosine"
  warmup_epochs: 0

# Fault injection (disabled - this is SAT)
activation_fault_injection:
  enabled: false
  probability: 0.0
  injection_type: "random"
  apply_during: "eval"
  target_layers: ["QuantReLU"]
  track_statistics: false
  verbose: false

weight_fault_injection:
  enabled: false

# AMP configuration
amp:
  enabled: false

# TensorBoard
tensorboard:
  enabled: true

# Progress bar
progress:
  enabled: true

# Checkpoint configuration
checkpoint:
  enabled: true
  dir: "./experiments"
  save_frequency: 10
  save_best: true
  # Load FAT checkpoint
  resume: "/path/to/pretrained_fat_model/checkpoint.pt"

# Multi-phase configuration
phases:
  # Single phase: SAT training (no faults) on top of FAT model
  - name: "sat_clean_training"
    epochs: 350

    optimizer:
      learning_rate: 0.0001  # Low LR to preserve FAT-learned robustness

    scheduler:
      T_max: 50  # Cosine annealing over 50 epochs

    # No fault injection - this is SAT
    activation_fault_injection:
      enabled: false
      track_statistics: false

    weight_fault_injection:
      enabled: false
