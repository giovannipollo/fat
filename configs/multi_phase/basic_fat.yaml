# Basic Fault-Aware Fine-Tuning Configuration
# Trains normally for 200 epochs, then fine-tunes with fault injection for 50 epochs

# Seed configuration
seed:
  enabled: true
  value: 42
  deterministic: false

# Dataset configuration
dataset:
  name: "cifar10"
  root: "./data"
  download: true
  num_workers: 4

# Model configuration
model:
  name: "quant_cnv"

# Quantization settings
quantization:
  in_weight_bit_width: 8
  weight_bit_width: 4
  act_bit_width: 4

# Loss function
loss:
  name: "sqr_hinge"

# Base training configuration (applies to all phases unless overridden)
training:
  batch_size: 256

# Base optimizer configuration
optimizer:
  name: "adam"
  learning_rate: 0.01
  weight_decay: 0.0

# Base scheduler configuration
scheduler:
  name: "cosine"
  warmup_epochs: 0

weight_fault_injection:
  enabled: false

# AMP configuration
amp:
  enabled: false

# TensorBoard
tensorboard:
  enabled: true

# Progress bar
progress:
  enabled: true

# Checkpoint configuration
checkpoint:
  enabled: true
  dir: "./experiments"
  save_frequency: 50
  save_best: true

# Multi-phase configuration
phases:
  # Phase 1: Standard training (no faults)
  - name: "standard_training"
    epochs: 5
    scheduler:
      T_max: 5  # Match phase duration

  # Phase 2: Fault-aware fine-tuning
  - name: "fault_aware_finetuning"
    epochs: 5
    load_checkpoint: "best"  # Load best model from phase 1

    optimizer:
      learning_rate: 0.001  # Lower LR for fine-tuning

    scheduler:
      T_max: 5  # Match phase duration

    activation_fault_injection:
      enabled: true
      probability: 5.0
      apply_during: "train"  # Inject faults during training only
      target_layers: ["QuantReLU"]
      injection_type: "random"
      track_statistics: true
